<!doctype html>
<html>
  <head>
  <title>
    
      Learning distributions on compact support using Normalizing Flows | Dies, Das, Ananas
    
  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <!-- Use Atom -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Dies, Das, Ananas" />
  <!-- Use RSS-2.0 -->
  <!--<link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title="Dies, Das, Ananas | Machine Learning and Stuff."/>
  //-->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      });
  </script>
  <!-- Google Analytics -->
  <!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>
 -->
  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Learning distributions on compact support using Normalizing Flows | Dies, Das, Ananas</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Learning distributions on compact support using Normalizing Flows" />
<meta name="author" content="Sven Elflein" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Normalizing Flows [Rezende and Mohamed 2015] are powerful density estimators that have shown to be able to learn complex distributions, e.g., of natural images [Kingma and Dhariwal 2018]." />
<meta property="og:description" content="Normalizing Flows [Rezende and Mohamed 2015] are powerful density estimators that have shown to be able to learn complex distributions, e.g., of natural images [Kingma and Dhariwal 2018]." />
<link rel="canonical" href="http://localhost:4000/normalizing_flow_bounded_domain" />
<meta property="og:url" content="http://localhost:4000/normalizing_flow_bounded_domain" />
<meta property="og:site_name" content="Dies, Das, Ananas" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-10T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Learning distributions on compact support using Normalizing Flows" />
<script type="application/ld+json">
{"@type":"BlogPosting","description":"Normalizing Flows [Rezende and Mohamed 2015] are powerful density estimators that have shown to be able to learn complex distributions, e.g., of natural images [Kingma and Dhariwal 2018].","headline":"Learning distributions on compact support using Normalizing Flows","dateModified":"2022-01-10T00:00:00+01:00","datePublished":"2022-01-10T00:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/normalizing_flow_bounded_domain"},"author":{"@type":"Person","name":"Sven Elflein"},"url":"http://localhost:4000/normalizing_flow_bounded_domain","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

  <body>
    <div class="container">
      <header class="header">
  <h3 class="header-title">
    <a href="/">Dies, Das, Ananas</a>
    <small class="header-subtitle">Machine Learning and Stuff.</small>
    <div class="menu">
  <nav class="menu-content">
    
      <a href="/about.html">About</a>
    
  </nav>
  <nav class="social-icons">
    
  
  
    <a href="https://www.github.com/selflein" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/s_elflein" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="http://www.linkedin.com/in/svenelflein/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:elflein.sven@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  
  
    <a href="/feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  </nav>
</div>

  </h3>
</header>

      <div class="content-container">
        <h1>
  Learning distributions on compact support using Normalizing Flows
</h1>
<article>
  <p>Normalizing Flows <a class="citation" href="#pmlr-v37-rezende15">[Rezende and Mohamed 2015]</a> are powerful density estimators that have shown to be able to learn complex distributions, e.g., of natural images <a class="citation" href="#NEURIPS2018-d139db6a">[Kingma and Dhariwal 2018]</a>.</p>

<p>Recently, I was interested in learning a distribution on line segments which only has compact support, i.e., the support is not $\mathbb{R}$ but only defined on a compact interval $[a, b]$ along the line segment.</p>

<p>The vanilla formulation of Normalizing Flows <a class="citation" href="#pmlr-v37-rezende15">[Rezende and Mohamed 2015]</a> only considers distributions with support in $\mathbb{R}$, and a quick literature research did not yield any solutions to the problem. By dwelling on this problem for a bit, I came up with a solution by carefully applying invertible transformations.</p>

<h2 id="the-idea">The idea</h2>

<p>Consider a vanilla normalizing flow stacking a set of invertible and differentiable transformations $\{f_1, \dots, f_n \}$. After applying common transformations (e.g. radial <a class="citation" href="#pmlr-v37-rezende15">[Rezende and Mohamed 2015]</a> or affine coupling <a class="citation" href="#dinh2015nice">[Dinh et al. 2015]</a> transform) the support of the function is still $\mathbb{R}$. This is visualized in Fig. 1.</p>

<figure class="image">
    <img src="../assets/img/normalizing_flow_bounded_domain_files/nf.png" alt="Figure 1: Common normalizing flow definition transforming a latent Normal distribution into a more complex, target distribution." width="" class="centerimg" />
    <figcaption>
        Figure 1: Common normalizing flow definition transforming a latent Normal distribution into a more complex, target distribution.
    </figcaption>
</figure>

<p>Now, in order to obtain a distribution with compact support we require a function that is invertible, differentiable (in order to satisfy the constrains within normalizing flows), and additionally we want the function to have a compact co-domain. One such choice, is the logistic function</p>

\[f_{n+1}: \mathbb{R} \mapsto [0, 1], f_{n+1}(x) = \frac{1}{1 + e^{-x}}\]

<p>which is visualized as transformation in the first part of Fig. 2.</p>

<figure class="image">
    <img src="../assets/img/normalizing_flow_bounded_domain_files/compact_transform.png" alt="Figure 2: Two additional transforms to squash the distribution into the [0, 1] interval and scale and move it afterwards." width="" class="centerimg" />
    <figcaption>
        Figure 2: Two additional transforms to squash the distribution into the [0, 1] interval and scale and move it afterwards.
    </figcaption>
</figure>

<p>After applying the logistic function, we can use a simple affine transformation $f_{n+2}$ in order to move and scale the support $[0, 1]$ to our desired interval as shown in the second part of Fig. 2.</p>

<h2 id="action">Action!</h2>

<p>Next, we are going to use the <a href="https://pyro.ai/">Pyro</a> library which itself is based on PyTorch to implement our idea and test the implementation by learning a simple 1D distribution with compact support.</p>

<p>In order to be able to learn the parameters of the normalizing flow efficiently using maximum likelihood, we <strong>need to be able to evaluate the likelihood of individual samples of our dataset</strong>. Therefore, we are going to use the <em>inverse</em> parameterization which allows us to transform our training sample backwards through the transformation shown in Fig. 1 and 2 in order to evaluate the density of the sample in latent distribution.</p>

<p>Now let us define the model in code:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NormalizingFlowDensity</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">flow_length</span><span class="p">,</span> <span class="n">flow_type</span><span class="o">=</span><span class="s">"radial_flow"</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NormalizingFlowDensity</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">flow_length</span> <span class="o">=</span> <span class="n">flow_length</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">flow_type</span> <span class="o">=</span> <span class="n">flow_type</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cov</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dim</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        
        <span class="n">modules</span> <span class="o">=</span> <span class="p">[</span>
            <span class="c1"># Affine transformation of the [0, 1] interval 
</span>            <span class="n">InvAffineTransformModule</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">loc</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">scale</span><span class="p">),</span>
            <span class="c1"># Squeeze R into [0, 1] interval
</span>            <span class="n">InvSigmoidTransform</span><span class="p">()</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">flow_type</span> <span class="o">==</span> <span class="s">"radial_flow"</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="n">modules</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span>
                <span class="p">[</span><span class="n">Radial</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">flow_length</span><span class="p">)]</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">NotImplementedError</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">modules</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">sum_log_jacobians</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">transform</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">transforms</span><span class="p">:</span>
            <span class="n">z_next</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            <span class="n">sum_log_jacobians</span> <span class="o">+=</span> <span class="n">transform</span><span class="p">.</span><span class="n">log_abs_det_jacobian</span><span class="p">(</span>
                <span class="n">z</span><span class="p">,</span> <span class="n">z_next</span>
            <span class="p">)</span>
            <span class="n">z</span> <span class="o">=</span> <span class="n">z_next</span>
        <span class="k">return</span> <span class="n">z</span><span class="p">,</span> <span class="n">sum_log_jacobians</span>

    <span class="k">def</span> <span class="nf">log_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span><span class="p">,</span> <span class="n">sum_log_jacobians</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">log_prob_z</span> <span class="o">=</span> <span class="n">tdist</span><span class="p">.</span><span class="n">MultivariateNormal</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">cov</span><span class="p">).</span><span class="n">log_prob</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">log_prob_x</span> <span class="o">=</span> <span class="n">log_prob_z</span> <span class="o">+</span> <span class="n">sum_log_jacobians</span>
        <span class="k">return</span> <span class="n">log_prob_x</span>
</code></pre></div></div>

<p>Note that since we want to use <em>inverse</em> parameterization, we add the inverse of the transforms in reverse order into the list. Further, we can set the support of the distribution using the parameters <code class="language-plaintext highlighter-rouge">loc</code> and <code class="language-plaintext highlighter-rouge">scale</code>.</p>

<p>We can use the <code class="language-plaintext highlighter-rouge">log_prob</code> function, which takes a datapoint $x$, computes the inverse transformation and returns the log likelihood for that datapoint $\log p(x)$ for training.</p>

<p>Now, we consider the following 1D example distribution which is a piecewise uniform with support $[1.0, 2.5]$ shown in Fig. 3.</p>

<figure class="image">
    <img src="../assets/img/normalizing_flow_bounded_domain_files/normalizing_flow_bounded_domain_11_1.png" alt="Figure 3: The target distribution we are aiming to learn on data." width="85%" class="centerimg" />
    <figcaption>
        Figure 3: The target distribution we are aiming to learn on data.
    </figcaption>
</figure>

<p>In order to learn the parameters $\theta$ of the normalizing flow, we can simply maximize the likelihood</p>

\[\arg\max_{\theta} p_{\theta}(x)\]

<p>which corresponds the following training code where we appropriately for the target distribution set the <code class="language-plaintext highlighter-rouge">loc</code> parameter to $1$ and the <code class="language-plaintext highlighter-rouge">scale</code> parameter to $1.5$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="n">NormalizingFlowDensity</span><span class="p">(</span>
    <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">flow_length</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">flow_type</span><span class="o">=</span><span class="s">"radial_flow"</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">)</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">device</span> <span class="o">=</span> <span class="s">"cpu"</span>
<span class="n">net</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">epoch_iter</span> <span class="o">=</span> <span class="n">trange</span><span class="p">(</span><span class="n">epochs</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">epoch_iter</span><span class="p">:</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">log_prob</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_prob</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    
    <span class="n">epoch_iter</span><span class="p">.</span><span class="n">set_description</span><span class="p">(</span><span class="sa">f</span><span class="s">"Loss: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">losses</span><span class="p">):.</span><span class="mi">03</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

</code></pre></div></div>

<p>Finally, we can plot the learned density in Fig. 4. Note that the density is only defined in the interval $[1.0, 2.5]$, however points outside the interval evaluate to $0$ due to clamping by Pyro.</p>

<figure class="image">
    <img src="../assets/img/normalizing_flow_bounded_domain_files/normalizing_flow_bounded_domain_16_1.png" alt="Figure 4: The distribution learned by our normalizing flow model." width="85%" class="centerimg" />
    <figcaption>
        Figure 4: The distribution learned by our normalizing flow model.
    </figcaption>
</figure>

<p>While the result is not perfect this gives a powerful framework for learning distributions on compact support. One can probably improve the result quite a bit by using more powerful transformations in the “base” flow (in fact one can use any existing invertible and differentiable transformation!) and by increasing the depth of the flow.</p>

<h2 id="conclusion">Conclusion</h2>
<p>Overall, we have seen how one can learn a distribution with compact support using normalizing flows by leveraging some simple transformations in the final layers and demonstrated some proof-of-concept results on a 1D toy example.</p>

<p>Feel free to check out the full notebook on which this blog post is based on <a href="https://gist.github.com/selflein/d8ff4b40142b5b8c4b32775fd04d8797">here</a>.</p>

<h2 id="references">References</h2>
<ol class="bibliography"><li><span id="pmlr-v37-rezende15"><span style="font-variant: small-caps">Rezende, D. and Mohamed, S.</span> 2015. Variational Inference with Normalizing Flows. <i>Proceedings of the 32nd International Conference on Machine Learning</i>, PMLR, 1530–1538.</span></li>
<li><span id="NEURIPS2018-d139db6a"><span style="font-variant: small-caps">Kingma, D.P. and Dhariwal, P.</span> 2018. Glow: Generative Flow with Invertible 1x1 Convolutions. <i>Advances in Neural Information Processing Systems</i>, Curran Associates, Inc.</span></li>
<li><span id="dinh2015nice"><span style="font-variant: small-caps">Dinh, L., Krueger, D., and Bengio, Y.</span> 2015. NICE: Non-linear Independent Components Estimation. .</span></li></ol>

</article>

  <span class="post-date">
  Written on
  
  January
  10th,
  2022
  by
  
    Sven Elflein
  
</span>



  <div class="post-date">Feel free to share!</div>
<div class="sharing-icons">
  <a href="https://twitter.com/intent/tweet?text=Learning distributions on compact support using Normalizing Flows&amp;url=/normalizing_flow_bounded_domain" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  <a href="https://www.facebook.com/sharer/sharer.php?u=/normalizing_flow_bounded_domain&amp;title=Learning distributions on compact support using Normalizing Flows" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
</div>



  <div class="related">
  <h1 >You may also enjoy:</h1>
  
  <ul class="related-posts">
    
      
        
        
      
    
      
        
          <li>
            <h3>
              <a href="/diffusion_practical_guide">
                Practical guide to Diffusion models
                <!--<img src="http://localhost:4000/images/">-->
                <!--<small>November 18, 2022</small>-->
              </a>
            </h3>
          </li>
          
        
      
        
        
      
        
          <li>
            <h3>
              <a href="/restricting-the-flow-review">
                Why did my Neural Network do that?
                <!--<img src="http://localhost:4000/images/">-->
                <!--<small>August 12, 2020</small>-->
              </a>
            </h3>
          </li>
          
        
      
    
  </ul>
</div>




      </div>
      <footer class="footer">
  
  
  
    <a href="https://www.github.com/selflein" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/s_elflein" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="http://www.linkedin.com/in/svenelflein/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:elflein.sven@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  
  
    <a href="/feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  <div class="footer-description"><a href="/">Dies, Das, Ananas | Machine Learning and Stuff. by Sven Elflein</a></div>
</footer>

    </div>
  </body>
</html>
