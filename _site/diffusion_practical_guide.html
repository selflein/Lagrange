<!doctype html>
<html>
  <head>
  <title>
    
      Practical guide to Diffusion models | Dies, Das, Ananas
    
  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">
  <!-- Use Atom -->
  <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Dies, Das, Ananas" />
  <!-- Use RSS-2.0 -->
  <!--<link href="/rss-feed.xml" type="application/rss+xml" rel="alternate" title="Dies, Das, Ananas | Machine Learning and Stuff."/>
  //-->
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      });
  </script>
  <!-- Google Analytics -->
  <!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>
 -->
  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Practical guide to Diffusion models | Dies, Das, Ananas</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Practical guide to Diffusion models" />
<meta name="author" content="Sven Elflein" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The motivation of this blog post is to provide a intuition and a practical guide to train a (simple) diffusion model [Sohl-Dickstein et al. 2015] together with the respective code leveraging PyTorch. If you are interested in a more mathematical description with proofs I can highly recommend [Luo 2022]." />
<meta property="og:description" content="The motivation of this blog post is to provide a intuition and a practical guide to train a (simple) diffusion model [Sohl-Dickstein et al. 2015] together with the respective code leveraging PyTorch. If you are interested in a more mathematical description with proofs I can highly recommend [Luo 2022]." />
<link rel="canonical" href="http://localhost:4000/diffusion_practical_guide" />
<meta property="og:url" content="http://localhost:4000/diffusion_practical_guide" />
<meta property="og:site_name" content="Dies, Das, Ananas" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-11-18T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Practical guide to Diffusion models" />
<script type="application/ld+json">
{"@type":"BlogPosting","description":"The motivation of this blog post is to provide a intuition and a practical guide to train a (simple) diffusion model [Sohl-Dickstein et al. 2015] together with the respective code leveraging PyTorch. If you are interested in a more mathematical description with proofs I can highly recommend [Luo 2022].","headline":"Practical guide to Diffusion models","dateModified":"2022-11-18T00:00:00+01:00","datePublished":"2022-11-18T00:00:00+01:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/diffusion_practical_guide"},"author":{"@type":"Person","name":"Sven Elflein"},"url":"http://localhost:4000/diffusion_practical_guide","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

  <body>
    <div class="container">
      <header class="header">
  <h3 class="header-title">
    <a href="/">Dies, Das, Ananas</a>
    <small class="header-subtitle">Machine Learning and Stuff.</small>
    <div class="menu">
  <nav class="menu-content">
    
      <a href="/about.html">About</a>
    
  </nav>
  <nav class="social-icons">
    
  
  
    <a href="https://www.github.com/selflein" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/s_elflein" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="http://www.linkedin.com/in/svenelflein/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:elflein.sven@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  
  
    <a href="/feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  </nav>
</div>

  </h3>
</header>

      <div class="content-container">
        <h1>
  Practical guide to Diffusion models
</h1>
<article>
  <p>The motivation of this blog post is to provide a intuition and a practical guide to train a (simple) diffusion model <a class="citation" href="#sohl2015deep">[Sohl-Dickstein et al. 2015]</a> together with the respective code leveraging PyTorch. If you are interested in a more mathematical description with proofs I can highly recommend <a class="citation" href="#luoUnderstandingDiffusionModels2022a">[Luo 2022]</a>.</p>

<p>In general, the goal of a diffusion model is to be able to generate novel data after being trained on data points of that distribution.</p>

<p>Here, let’s consider a simple 2D toy dataset provided by scikit-learn to make this example as simple as possible:</p>

<figure class="image">
    <img src="../assets/img/diffusion_practical_guide_files/dataset.png" alt="Figure 1: Two Moons toy dataset used for our experiments." width="50%" class="centerimg" />
    <figcaption>
        Figure 1: Two Moons toy dataset used for our experiments.
    </figcaption>
</figure>

<p>Diffusion models define a forward and backward process:</p>

<ul>
  <li>the forward process gradually adds noise to the data until the original data is indistinguishable (one arrives at a standard normal distribution $N(0, \mathbf{I})$)</li>
  <li>the backward process aims to reverse the forward process, i.e., start from noise and then gradually try to restore data</li>
</ul>

<p>One aims to learn the backward process, so one can generate new samples by starting from random noise.</p>

<p>To be able to start training a model that learns this backward process, we need to know how to do the forward process one aims to reverse.</p>

<p>The Gaussian noise one adds at every step $t$ is controlled by parameters $\beta_t$ that increase as $t \rightarrow T$ so we get random Gaussian noise in the end:</p>

\[\begin{equation}
q(x_t \mid x_{t-1}) \sim \mathcal{N}(\sqrt{1 - \beta_t}x_{t-1}, \beta_t \mathbf{I})
\end{equation}\]

<p>So one can start from our original data samples $x_0$ and then gradually adds noise to the samples.</p>

<figure class="image">
    <img src="../assets/img/diffusion_practical_guide_files/forward_diffusion.png" alt="Figure 2: Forward diffusion process that gradually adds noise." width="110%" class="centerimg" />
    <figcaption>
        Figure 2: Forward diffusion process that gradually adds noise.
    </figcaption>
</figure>

<p>The cool thing about this being Gaussian noise is that instead of simulating this forward process by iteratively sampling noise, one can derive a closed form for the distribution at a certain $t$ given the original data point $x_0$ so one has to only sample noise once:</p>

\[\begin{equation}
q(x_t \mid x_0) \sim \mathcal{N}(\sqrt{\bar{\alpha}}_t x_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{equation}\]

<p>with $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}_t = \prod_{s = 1}^t \alpha_s$.</p>

<p>Let’s implement this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ForwardProcess</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">betas</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">betas</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">betas</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">alpha_bar</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cumprod</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">alphas</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">get_x_t</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_0</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""Forward diffusion process given the unperturbed sample x_0.
        
        Args:
            x_0: Original, unperturbed samples.
            t: Target timestamp of the diffusion process of each sample.
        
        Returns:
            Noise added to original sample and perturbed sample.
        """</span>
        <span class="n">eps_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_0</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">x_0</span><span class="p">)</span>
        <span class="n">alpha_bar</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha_bar</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha_bar</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_0</span>
        <span class="n">std</span> <span class="o">=</span> <span class="p">((</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">alpha_bar</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">eps_0</span><span class="p">,</span> <span class="n">mean</span> <span class="o">+</span> <span class="n">std</span> <span class="o">*</span> <span class="n">eps_0</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="training">Training</h3>
<p>Next, we want to train a model that reverses that process.</p>

<p>For this, one can show that the there is also a closed form for the less noisy version $x_{t-1}$ given the next sample $x_t$ and the original sample $x_0$.</p>

\[\tag{1}\label{eq:reverse}
\begin{equation}
q(x_{t-1} \mid x_t, x_0) = \mathcal{N}(\mu(x_t, x_0), \sigma_t^2\mathbf{I}
\end{equation}\]

<p>where</p>

\[\begin{equation}
\sigma_t^2 = \frac{(1 - \alpha_t)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}, \quad \mu(x_t, x_0) = \frac{1}{\sqrt{\alpha_t}} \left(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_0\right)
\end{equation}\]

<p>and $\epsilon_0 \sim \mathcal{N}(0, \mathbf{I})$ is the noise drawn to perturb the original data $x_0$<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<p>Obviously, we cannot use this directly to generate new data since this relies on knowing the original datapoint $x_0$ in the first place but <strong>we can use it to generate the ground truth data for training a model that does not rely on $\mathbf{x}_0$ and predicts $\epsilon_0$ from the noisy data $\mathbf{x}_t$ and $t$ alone</strong><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<p>Let’s define a small neural network $\epsilon_{\mathbf{\theta}}(\mathbf{x}_t, t)$ where $\mathbf{\theta}$ are the parameters of the network that does just that:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NoisePredictor</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">t_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>   <span class="c1"># Input: Noisy data x_t and t
</span>            <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">LeakyReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
            <span class="c1"># Output: Predicted noise that was added to the original data point
</span>            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="c1"># Encode the time index t as one-hot and then use one layer to encode
</span>        <span class="c1"># into a single value
</span>        <span class="n">t_embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">t_encoder</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">T</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
        <span class="p">)</span>
        
        <span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t_embedding</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
</code></pre></div></div>

<p>Here, we encode the timestamp of the diffusion process $t$ as a one-hot vector with a single layer and then concatenate this information with the noisy data.</p>

<p><strong>Next up</strong>: Training the model to predict the noise. 
For this, one can just sample $t$’s, use the forward process to generate the noisy sample $x_t$ together with the noise $e_0$, and train the model to reduce the mean squared error between the predicted noise and $e_0$.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">NoisePredictor</span><span class="p">(</span><span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="mi">5000</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="c1"># Sample random t's
</span>        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="n">T</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">N</span><span class="p">,))</span>

        <span class="c1"># Get the noise added and the noisy version of the data using the forward
</span>        <span class="c1"># process given t
</span>        <span class="n">eps_0</span><span class="p">,</span> <span class="n">x_t</span> <span class="o">=</span> <span class="n">fp</span><span class="p">.</span><span class="n">get_x_t</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>
    
    <span class="c1"># Predict the noise added to x_0 from x_t
</span>    <span class="n">pred_eps</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

    <span class="c1"># Simplified objective without weighting with alpha terms (Ho et al, 2020)
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">pred_eps</span><span class="p">,</span> <span class="n">eps_0</span><span class="p">)</span>

    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="inference">Inference</h3>
<p>After training the model to predict the noise $\epsilon$, we can simply iteratively run the reverse process to predict $\mathbf{x}_{t-1}$ from $x_t$ starting from random noise $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$ as defined in \eqref{eq:reverse} where we set the mean:</p>

\[\begin{equation}
\mu(x_t) = \frac{1}{\sqrt{\alpha_t}} \left(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_{\mathbf{\theta}}(\mathbf{x}_t, t) \right)
\end{equation}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ReverseProcess</span><span class="p">(</span><span class="n">ForwardProcess</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">betas</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">betas</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">betas</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">alphas</span><span class="p">)</span>
            <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="n">roll</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">alpha_bar</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha_bar</span><span class="p">)</span>
        <span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>
    
    <span class="k">def</span> <span class="nf">get_x_t_minus_one</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_t</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">t</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">t_vector</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x_t</span><span class="p">),),</span> <span class="n">fill_value</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t_vector</span><span class="p">)</span>
        
        <span class="n">eps</span> <span class="o">*=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">alphas</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">/</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="p">.</span><span class="n">alpha_bar</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
        <span class="n">mean</span> <span class="o">=</span>  <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">alphas</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x_t</span> <span class="o">-</span> <span class="n">eps</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mean</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigma</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_t</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">full_trajectory</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="c1"># Initialize with X_T ~ N(0, I)
</span>        <span class="n">x_t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">trajectory</span> <span class="o">=</span> <span class="p">[</span><span class="n">x_t</span><span class="p">.</span><span class="n">clone</span><span class="p">()]</span>
        
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">get_x_t_minus_one</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>
            
            <span class="k">if</span> <span class="n">full_trajectory</span><span class="p">:</span>
                <span class="n">trajectory</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_t</span><span class="p">.</span><span class="n">clone</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">trajectory</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">if</span> <span class="n">full_trajectory</span> <span class="k">else</span> <span class="n">x_t</span>
</code></pre></div></div>

<p>Now, let’s sample new data points and plot them:</p>

<figure class="image">
    <img src="../assets/img/diffusion_practical_guide_files/new_samples.png" alt="Figure 3: New samples generated from the trained diffusion model." width="50%" class="centerimg" />
    <figcaption>
        Figure 3: New samples generated from the trained diffusion model.
    </figcaption>
</figure>

<p>We can also inspect the (negative) direction of the predicted noise vector at a particular timestamp $t$ for each position in a grid to visualize the dynamics a sample follows during the reverse process as a vector field:</p>

<figure class="image">
    <img src="../assets/img/diffusion_practical_guide_files/vectorfield.png" alt="Figure 4: Vector field describing reverse process dynamics at different timestamps. The blue line shows the trajectory of a sample during the reverse process." width="100%" class="centerimg" />
    <figcaption>
        Figure 4: Vector field describing reverse process dynamics at different timestamps. The blue line shows the trajectory of a sample during the reverse process.
    </figcaption>
</figure>

<p>One can see that as $t \rightarrow 0$ more fine-grained structure emerges that guides the sample to the original data manifold. At $t=T$ there the (pure) noise is just guided towards the center as the signal is still very low for the network to predict.</p>

<h2 id="insights">Insights</h2>

<p>Working on this small dataset already revealed some important things that one has to consider when training diffusion models.
In particular, in the beginning when I started to implement this from the paper description, a huge amount of diffusion steps ($T=1000$) were required to yield good results.</p>

<p>Further looking into the literature and appendix of the papers revealed some things that brought down the diffusion steps required to $T=10$:</p>
<ul>
  <li>It is important to perform linear scaling of the input data into the range $[-1, 1]$. Standardizing the input data (i.e., subtracting the mean and dividing by the standard dev.) as it is usually done for neural networks yielded worse results</li>
  <li>The Variance schedule (${\beta_t}_t$) ideally has small changes towards $t=0$ such that the noise is not too much for the network to reconstruct, i.e., it learn fine-grained details of the data. This was already discovered in <a class="citation" href="#nichol2021improved">[Nichol and Dhariwal 2021]</a>, however, it is interesting to see that his insight can be shown from a toy dataset already instead of training expensive image models. Fig. 5 shows how the variance of the forward process $1 - \bar{\alpha}_t$ evolves for when $\beta_t$ is set linear (left), or polynomial (right). The right setting works much better in practice since the perturbation of the input does not happen too fast.</li>
</ul>

<figure class="image">
    <img src="../assets/img/diffusion_practical_guide_files/variance_schedule.png" alt="Figure 5: Different variance schedules for the diffusion process." width="100%" class="centerimg" />
    <figcaption>
        Figure 5: Different variance schedules for the diffusion process.
    </figcaption>
</figure>

<p>Check out the full notebook which this blog post is based on <a href="https://gist.github.com/selflein/9bee0818a48966179b18d577a89f792a">here</a>.</p>

<h2 id="references">References</h2>
<ol class="bibliography"><li><span id="sohl2015deep"><span style="font-variant: small-caps">Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S.</span> 2015. Deep unsupervised learning using nonequilibrium thermodynamics. <i>International Conference on Machine Learning</i>, PMLR, 2256–2265.</span></li>
<li><span id="luoUnderstandingDiffusionModels2022a"><span style="font-variant: small-caps">Luo, C.</span> 2022. Understanding Diffusion Models: A Unified Perspective. .</span></li>
<li><span id="nichol2021improved"><span style="font-variant: small-caps">Nichol, A.Q. and Dhariwal, P.</span> 2021. Improved Denoising Diffusion Probabilistic Models. <i>International Conference on Machine Learning</i>, PMLR, 8162–8171.</span></li></ol>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This is one possible parameterization of the mean that is most effective based on the experiments in <a class="citation" href="#nichol2021improved">[Nichol and Dhariwal 2021]</a>. <a class="citation" href="#luoUnderstandingDiffusionModels2022a">[Luo 2022]</a> summarizes two other paramterizations in the literature, e.g., regressing the mean directly. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Here we treat the variances as fixed. <a class="citation" href="#nichol2021improved">[Nichol and Dhariwal 2021]</a> propose to learn these with an additional objective. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

</article>

  <span class="post-date">
  Written on
  
  November
  18th,
  2022
  by
  
    Sven Elflein
  
</span>



  <div class="post-date">Feel free to share!</div>
<div class="sharing-icons">
  <a href="https://twitter.com/intent/tweet?text=Practical guide to Diffusion models&amp;url=/diffusion_practical_guide" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  <a href="https://www.facebook.com/sharer/sharer.php?u=/diffusion_practical_guide&amp;title=Practical guide to Diffusion models" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
</div>



  <div class="related">
  <h1 >You may also enjoy:</h1>
  
  <ul class="related-posts">
    
      
        
        
      
    
      
        
        
      
        
          <li>
            <h3>
              <a href="/normalizing_flow_bounded_domain">
                Learning distributions on compact support using Normalizing Flows
                <!--<img src="http://localhost:4000/images/">-->
                <!--<small>January 10, 2022</small>-->
              </a>
            </h3>
          </li>
          
        
      
        
          <li>
            <h3>
              <a href="/restricting-the-flow-review">
                Why did my Neural Network do that?
                <!--<img src="http://localhost:4000/images/">-->
                <!--<small>August 12, 2020</small>-->
              </a>
            </h3>
          </li>
          
        
      
    
  </ul>
</div>




      </div>
      <footer class="footer">
  
  
  
    <a href="https://www.github.com/selflein" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>
  

  
  
    <a href="https://twitter.com/s_elflein" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
  

  
  
    <a href="http://www.linkedin.com/in/svenelflein/" target="_blank"><i class="fa fa-linkedin" aria-hidden="true"></i></a>
  

  
  
    <a href="mailto:elflein.sven@gmail.com" target="_blank"><i class="fa fa-envelope" aria-hidden="true"></i></a>
  

  
  
    <a href="/feed.xml"><i class="fa fa-rss-square" aria-hidden="true"></i></a>
  

  <div class="footer-description"><a href="/">Dies, Das, Ananas | Machine Learning and Stuff. by Sven Elflein</a></div>
</footer>

    </div>
  </body>
</html>
